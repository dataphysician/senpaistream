{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers pillow numpy tqdm\n",
    "# !pip install pdf2image\n",
    "# !apt-get install -y poppler-utils\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDF to a list of Image files\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "!wget -O enbj001.zip https://densho810.com/free/dl/enbj001.zip\n",
    "!unzip enbj001.zip -d .\n",
    "\n",
    "pdf_path = './enbj001/enbj01.pdf' # TODO: Path to upload\n",
    "images = convert_from_path(pdf_path, dpi=150) # or 300 dpi if memory permits\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    image.save(f'page_{i + 1}.png', 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "# Ensure model runs on GPU if available, else fallback to CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' if device == \"cuda\" else \"-1\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModel.from_pretrained(\"ragavsachdeva/magiv2\", trust_remote_code=True, cache_dir=\"./cache\").to(device).eval()\n",
    "\n",
    "# Function to read images\n",
    "def read_image(path_to_image):\n",
    "    if not os.path.exists(path_to_image):\n",
    "        print(f\"‚ö†Ô∏è Warning: Image {path_to_image} not found!\")\n",
    "        return None\n",
    "    with open(path_to_image, \"rb\") as file:\n",
    "        image = Image.open(file).convert(\"L\").convert(\"RGB\")\n",
    "        return np.array(image)\n",
    "\n",
    "# Process Chapter Pages\n",
    "num_pages = 60  # First chapter is 60 pages\n",
    "chapter_pages = [f\"page_{i+1}.png\" for i in range(num_pages)]\n",
    "\n",
    "# Check if all pages exist\n",
    "missing_pages = [p for p in chapter_pages if not os.path.exists(p)]\n",
    "if missing_pages:\n",
    "    print(f\"‚ùå Missing pages: {missing_pages}\")\n",
    "    raise FileNotFoundError(\"Some pages are missing! Ensure all pages exist before running.\")\n",
    "\n",
    "# Load character images (need to change this to work)\n",
    "character_bank = {\n",
    "    \"images\": [\"announcer.png\", \"director.png\", \"hurt_patient1.png\", \"hurt_patient2.png\", \n",
    "               \"nurse-a1.png\", \"nurse-a2.png\", \"nurse-b1.png\", \"nurse-b2.png\", \"nurse-b3.png\", \n",
    "               \"dr_ushida1.png\", \"dr_ushida2.png\", \"dr_ushida3.png\", \"dr_ushida7.png\", \n",
    "               \"dr_ushida10.png\", \"dr_saito1.png\", \"dr_saito3.png\", \"dr_saito7.png\", \n",
    "               \"dr_saito9.png\", \"dr_saito12.png\", \"dr_saito14.png\"],  \n",
    "    \"names\": [\"Announcer\", \"Senior Surgeon\", \"Badly Injured Patient\", \"Badly Injured Patient\", \n",
    "              \"Nurse 1\", \"Nurse 1\", \"Nurse 2\", \"Nurse 2\", \"Nurse 2\", \"Dr. Ushida\", \"Dr. Ushida\", \n",
    "              \"Dr. Ushida\", \"Dr. Ushida\", \"Dr. Ushida\", \"Dr. Saito\", \"Dr. Saito\", \"Dr. Saito\", \n",
    "              \"Dr. Saito\", \"Dr. Saito\", \"Dr. Saito\"]\n",
    "}\n",
    "\n",
    "# Convert character images\n",
    "character_bank[\"images\"] = [read_image(x) for x in character_bank[\"images\"] if os.path.exists(x)]\n",
    "\n",
    "# Read pages into memory\n",
    "chapter_pages = [read_image(x) for x in chapter_pages if os.path.exists(x)]\n",
    "\n",
    "# Run model prediction\n",
    "print(\"üîÑ Processing pages with AI model...\")\n",
    "with torch.no_grad():\n",
    "    per_page_results = model.do_chapter_wide_prediction(chapter_pages, character_bank, use_tqdm=True, do_ocr=True)\n",
    "\n",
    "# Generate transcript\n",
    "transcript = []\n",
    "for i, (image, page_result) in enumerate(zip(chapter_pages, per_page_results)):\n",
    "    model.visualise_single_image_prediction(image, page_result, f\"page_{i+1}.png\")\n",
    "\n",
    "    # Associate text with character names\n",
    "    speaker_name = {\n",
    "        text_idx: page_result[\"character_names\"][char_idx] \n",
    "        for text_idx, char_idx in page_result[\"text_character_associations\"]\n",
    "    }\n",
    "\n",
    "    # Extract essential dialogues\n",
    "    for j in range(len(page_result[\"ocr\"])):\n",
    "        if not page_result[\"is_essential_text\"][j]:\n",
    "            continue\n",
    "        name = speaker_name.get(j, \"unsure\") \n",
    "        transcript.append(f\"<{name}>: {page_result['ocr'][j]}\")\n",
    "    \n",
    "    print(f\"‚úÖ Page {i+1} processed.\")\n",
    "\n",
    "# Save transcript\n",
    "with open(\"transcript.txt\", \"w\", encoding=\"utf-8\") as t:\n",
    "    for line in transcript:\n",
    "        t.write(line + \"\\n\")\n",
    "\n",
    "print(\"üéâ Transcript processing complete!\")\n",
    "\n",
    "# Display transcript in Jupyter Notebook\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(\"\\n\".join(transcript[:50])))  # Show first 50 lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Transcript and Images\n",
    "\n",
    "import os, base64\n",
    "from textwrap import dedent\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Path to transcript\n",
    "transcript_path = \"./transcript.txt\"\n",
    "with open(transcript_path, 'r', encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "# Path to your image\n",
    "image_paths = [f\"./page_{i+1}.png\" for i in range(60)] # TODO: Dynamic Loading\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_images = [encode_image(image_path) for image_path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the OpenAI message\n",
    "from textwrap import dedent\n",
    "messages=[{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": dedent(\"\"\"You are a graphic novel storyteller specializing in Japanese manga.\n",
    "        Given the pages of a manga chapter and the transcription of the character speeches in the entire chapter, skillfully narrate the entire story portrayed by the sequence of image panels.\n",
    "        Allow the narrator to be as verbose and detailed as possible to properly describe the context in the image panels.\n",
    "        Include as many detailed descriptions of the scene as possible. There is more value in being able to reach 15 minutes long.\n",
    "        Introduce the personality of the characters by conveying the appropriate emotions of their spoken words.\n",
    "        Portray as vivid an imagery as possible, and generate a narrative with dynamic emotions.\n",
    "        Pay particular attention to the description of characters that are introduced into a scene without any speeches. It provides additional context to the panel.\n",
    "        For the format of the script, use the following <style>:\n",
    "            <Narrator>: \"Speech...\"\n",
    "            <Character>: \"Speech...\"\n",
    "        Use appropriate capitalization and punctuation when stronger emotional emphasis is needed.\n",
    "        Example:\n",
    "            <Character>: \"Wait..hold on. THAT'S NOT WHAT I SAID!...\"\n",
    "        Maintain a consistent name for all the <Characters>\n",
    "        Adhere to the temporality of events and spoken dialogue in the transcript.\n",
    "        The goal is to fuse together both text from the transcript, and flowery details from the panel drawings to convert a manga into an audiobook narrative.\n",
    "        The narrative will be sent to a TTS system like ElevenLabs.\"\"\")\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"||-TRANSCRIPT-||\\n{transcript}\",\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "messages[1]['content'].extend(\n",
    "    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}}\n",
    "    for image in base64_images\n",
    ")\n",
    "\n",
    "# import instructor\n",
    "# from typing import Literal\n",
    "# from pydantic import BaseModel, Field\n",
    "# client = instructor.from_openai(\n",
    "#     client=OpenAI(),\n",
    "#     mode=instructor.Mode.TOOLS,\n",
    "# )\n",
    "# class Speech(BaseModel):\n",
    "#     speaker: Literal['Dr. Saito', 'Dr. Ushida', 'Announcer', 'Senior Surgeon', 'Narrator', 'Other'] = Field(description=\"The name of the character delivering the dialogue or narration.\")\n",
    "#     dialogue: str = Field(description=\"The lines delivered by the speaker.\")\n",
    "\n",
    "# class Script(BaseModel):\n",
    "#     script: list[Speech]\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "response = client.chat.completions.create(\n",
    "    # model=\"gpt-4o-mini\",\n",
    "    model=\"o1\",\n",
    "    messages=messages,\n",
    "    # response_model=Script # TODO: Test with Structured Outputs\n",
    ")\n",
    "with open('final_transcript.txt', 'r', encoding='utf-8') as ft:\n",
    "    transcript = ft.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list(speaker, dialogue)\n",
    "import re\n",
    "\n",
    "raw_content = response.choices[0].message.content\n",
    "with open('final_transcript.txt', 'w', encoding='utf-8') as ft:\n",
    "    ft.write(raw_content)\n",
    "\n",
    "pattern = re.compile(r'<([^>]+)>:\\s*(.*?)(?=\\n\\n<|$)', re.DOTALL)\n",
    "script = pattern.findall(raw_content)\n",
    "speakers = {speaker for speaker, _ in script}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the script and play the output\n",
    "from dotenv import load_dotenv\n",
    "from elevenlabs.client import ElevenLabs\n",
    "from elevenlabs import play\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "speakers = {speaker for speaker, _ in script}\n",
    "\n",
    "speakers = {\n",
    "    'Dr. Saito': \"bIHbv24MWmeRgasZH58o\", # Will\n",
    "    'Dr. Ushida': \"iP95p4xoKVk53GoZ742B\", # Chris\n",
    "    'Announcer': \"CwhRBWXzGAHq8TQ4Fs17\", # Roger\n",
    "    'Senior Surgeon': \"N2lVS1w4EtoT3dr4eOWO\", # Callum\n",
    "    'Narrator': \"nPczCjzI2devNBz1zQrb\", # Brian\n",
    "    'Other': \"JBFqnCBsd6RMkjVDRZzb\", # George\n",
    "    'Nurse 1': \"cgSgspJ2msm6clMCkdW9\", # Jessica\n",
    "    'Nurse 2': \"cgSgspJ2msm6clMCkdW9\" # Jessica\n",
    "}\n",
    "\n",
    "client = ElevenLabs()\n",
    "\n",
    "def create_audio(speaker, text):\n",
    "    audio = client.text_to_speech.convert(\n",
    "        text= text,\n",
    "        voice_id= speakers[speaker] if speaker in speakers.keys() else \"JBFqnCBsd6RMkjVDRZzb\",\n",
    "        model_id=\"eleven_multilingual_v2\",\n",
    "        output_format=\"mp3_44100_128\",\n",
    "    )\n",
    "    return audio\n",
    "\n",
    "for speaker, dialogue in script:\n",
    "    audio = create_audio(speaker, dialogue)\n",
    "    from rich import print as rprint; rprint(f\"{speaker.upper()}:\\t{dialogue}\")\n",
    "    play(audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
